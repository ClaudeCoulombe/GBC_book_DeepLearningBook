{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The revenge of Perceptron - Learning XOR with TensorFlow\n",
    "## Inspired by the GBC Book or the Deep Learning Book\n",
    "### by Claude COULOMBE - TÉLUQ / UQAM - Montréal\n",
    "\n",
    "### Introduction\n",
    "\n",
    "The GBC book is worth the reading. It's definitely THE authoritative reference on Deep Learning but you should not be allergic to maths. \n",
    "\n",
    "That said, the main weakness of this masterpiece is the lack of practical programming exercices left to a companion web site. But to cover all the practical stuff, the book should have exceeded 775 pages that it already has. \n",
    "\n",
    "I dream of he same content in the form of a series of iPython Notebooks with all exercices and code samples using Keras, TensorFlow and Theano.\n",
    "\n",
    "So now, I commit a modest contribution to my dream by coding the 6.1 Example: Learning XOR pp. 166 to 171, using TensorFlow. Ok, I know I should have used Theano which is mainly developed and maintained by the MILA lab at UdeM... maybe later.\n",
    "\n",
    "[Goodfellow, Bengio, Courville, 2016] Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep learning. MIT Press. On line at: http://www.deeplearningbook.org/\n",
    "\n",
    "\n",
    "## 6.1 Example: Learning XOR - GBC Book - Chapter 6 - pp. 166 to 171\n",
    "\n",
    "### The XOR problem\n",
    "\n",
    "In order to make the idea of a feedforward network (or multilayer perceptron) more concrete, in chapter 6 (Deep Feedforward Networks) the GBC book suggests a small example of a fully functioning feedforward network on a very simple task: learning the XOR (“exclusive or”) function. Below, the XOR Truth Table:\n",
    "\n",
    "| $x_1$ | $x_2$ | $x_1$ XOR $x_2$ |\n",
    "|:-:|:-:|:-------:|\n",
    "| 0 | 0 |    0    |  \n",
    "| 0 | 1 |    1    |\n",
    "| 1 | 0 |    1    |\n",
    "| 1 | 1 |    0    |\n",
    "\n",
    "Nobody should get too excited, this is NOT a deep neural network example. It's quite the contrary, but it's an instructive illustration of a simple problem that requires a three layers perceptron (with a so-called \"hidden\" or \"intermediary\" layer) also known as one \"hidden layer\" perceptron. It also needs nonlinear functions called activation functions. \n",
    "\n",
    "The problem with XOR is that its outputs are not linearly separable. No unique straight line is able to separe the positive from the negative examples. \n",
    "\n",
    "<img src=\"images/XOR_not_linearly_separable.png\" width=500 />\n",
    "\n",
    "### The revenge of Perceptron\n",
    "\n",
    "#### One hidden layer Neural Network can learn XOR\n",
    "\n",
    "<u>That said, we will show now that a neural network with one hidden layer and the backpropagation algorithm can learn XOR.</u> \n",
    "\n",
    "Below the architecture of our shallow (not deep at all!) feedforward neural network which contains 5 neurons or units distributed on 3 layers: an input layer which contains two neurons (or units), a single hidden layer containing 2 neurons and the output layer with 1 neuron. \n",
    "\n",
    "<img src=\"images/feedforward_network_solving_XOR.png\" width=400 />\n",
    "\n",
    "The left part of the above figure shows a detailed representation of the neural network, neuron by neuron with all the connections (except the biases). Good for small NNs this notation can be too cumbersome for larger networks. At right, the neural network is presented by layers in a more compact notation with weights represented by matrix ($W$ and $w$).      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow implementation\n",
    "\n",
    "Now it's time to move on and implement the XOR network in TensorFlow. I've been inspired by the blog post <a href=\"https://aimatters.wordpress.com/2016/01/16/solving-xor-with-a-neural-network-in-tensorflow/\">\"Solving XOR with a Neural Network in TensorFlow\"</a> by Stephen OMAN and the <a href=\"https://github.com/StephenOman/TensorFlowExamples/blob/master/xor%20nn/xor_nn.py\">code</a> in his GitHub repo. \n",
    "\n",
    "#### Playing with different loss functions\n",
    "\n",
    "I also proposed different loss functions that I've commented in my code below. \n",
    "\n",
    "First, a naive direct implementation of the loss function as shown in the GBC book.\n",
    "\n",
    "`n_instances = X.get_shape().as_list()[0]`<br/>\n",
    "`loss = tf.reduce_sum(tf.pow(y_estimated - Y, 2))/ n_instances`\n",
    "\n",
    "Then the classical MSE function suggested for math simplicity in the GBC book which uses the TensorFlow `tf.reduce_mean` function that should take care of numerical stability issue as I read somewhere...<br/>       \n",
    "\n",
    "`loss = tf.reduce_mean(tf.squared_difference(y_estimated, Y))` \n",
    "\n",
    "For better result with binary classifier, use cross entropy with a sigmoid<br/>\n",
    "`loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=y_estimated, labels=Y)`\n",
    "\n",
    "In case of problem with gradient (exploding or vanishing gradient) we could alternatively perform gradient clipping using the TensorFlow function `tf.clip_by_value(t, clip_value_min, clip_value_max)`. Any value less than clip_value_min will be set to clip_value_min. Any value greater than clip_value_max will be set to clip_value_max.\n",
    "\n",
    "`loss = tf.reduce_sum(tf.pow(tf.clip_by_value(y_estimated,1e-10,1.0) - Y,2))/(n_instances)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "Epoch:  0\n",
      "   y_estimated: \n",
      "     [ 0.49961096]\n",
      "     [ 0.9245432]\n",
      "     [ 0.50023597]\n",
      "     [ 0.91048223]\n",
      "   W: \n",
      "     [-0.41441455 -0.10702395]\n",
      "     [-1.52169275  1.46376789]\n",
      "   c: \n",
      "     [ 0.  0.]\n",
      "     [ 0.          0.00033887]\n",
      "     [ 0.  0.]\n",
      "     [ 0.         -0.00099035]\n",
      "   w: \n",
      "     [-1.03013289]\n",
      "     [ 1.71131086]\n",
      "   b \n",
      "     [-0.00155615]\n",
      "     [ 0.00019796]\n",
      "     [ 0.00094385]\n",
      "     [-0.00057854]\n",
      "   loss:  [[ 0.97383487]\n",
      " [ 0.33412135]\n",
      " [ 0.47398791]\n",
      " [ 1.24861741]]\n",
      "________________________________________________________________________________\n",
      "Epoch:  10000\n",
      "   y_estimated: \n",
      "     [ 0.02192736]\n",
      "     [ 0.99700952]\n",
      "     [ 0.95889795]\n",
      "     [ 0.02208406]\n",
      "   W: \n",
      "     [-0.41441455 -0.8323673 ]\n",
      "     [-1.52169275  1.55813658]\n",
      "   c: \n",
      "     [ 0.  0.]\n",
      "     [ 0.          0.82005012]\n",
      "     [ 0.  0.]\n",
      "     [ 0.         -0.72633374]\n",
      "   w: \n",
      "     [-1.03013289]\n",
      "     [ 2.25646877]\n",
      "   b \n",
      "     [-3.79784894]\n",
      "     [ 0.44301692]\n",
      "     [ 3.14972639]\n",
      "     [-3.79056764]\n",
      "   loss:  [[ 0.70417094]\n",
      " [ 0.31406686]\n",
      " [ 0.32448286]\n",
      " [ 0.70425016]]\n",
      "________________________________________________________________________________\n",
      "Epoch:  20000\n",
      "   y_estimated: \n",
      "     [ 0.01058332]\n",
      "     [ 0.99870241]\n",
      "     [ 0.98010433]\n",
      "     [ 0.01062038]\n",
      "   W: \n",
      "     [-0.41441455 -0.87230939]\n",
      "     [-1.52169275  1.63857651]\n",
      "   c: \n",
      "     [ 0.  0.]\n",
      "     [ 0.          0.94043136]\n",
      "     [ 0.  0.]\n",
      "     [ 0.         -0.76627582]\n",
      "   w: \n",
      "     [-1.03013289]\n",
      "     [ 2.38503313]\n",
      "   b \n",
      "     [-4.53783655]\n",
      "     [ 0.49490762]\n",
      "     [ 3.89715528]\n",
      "     [-4.53430367]\n",
      "   loss:  [[ 0.69845289]\n",
      " [ 0.31361085]\n",
      " [ 0.3186515 ]\n",
      " [ 0.69847143]]\n",
      "________________________________________________________________________________\n",
      "Epoch:  30000\n",
      "   y_estimated: \n",
      "     [ 0.00695167]\n",
      "     [ 0.99919838]\n",
      "     [ 0.9869467]\n",
      "     [ 0.00696777]\n",
      "   W: \n",
      "     [-0.41441455 -0.89429468]\n",
      "     [-1.52169275  1.68251872]\n",
      "   c: \n",
      "     [ 0.  0.]\n",
      "     [ 0.          1.00635421]\n",
      "     [ 0.  0.]\n",
      "     [ 0.         -0.78826112]\n",
      "   w: \n",
      "     [-1.03013289]\n",
      "     [ 2.4567585]\n",
      "   b \n",
      "     [-4.96179676]\n",
      "     [ 0.52213991]\n",
      "     [ 4.32557392]\n",
      "     [-4.95946789]\n",
      "   loss:  [[ 0.69662911]\n",
      " [ 0.31347734]\n",
      " [ 0.31678906]\n",
      " [ 0.69663715]]\n",
      "________________________________________________________________________________\n",
      "Epoch:  40000\n",
      "   y_estimated: \n",
      "     [ 0.00517067]\n",
      "     [ 0.99942857]\n",
      "     [ 0.99030262]\n",
      "     [ 0.00517954]\n",
      "   W: \n",
      "     [-0.41441455 -0.9093402 ]\n",
      "     [-1.52169275  1.71246386]\n",
      "   c: \n",
      "     [ 0.  0.]\n",
      "     [ 0.          1.05134523]\n",
      "     [ 0.  0.]\n",
      "     [ 0.         -0.80330664]\n",
      "   w: \n",
      "     [-1.03013289]\n",
      "     [ 2.50617933]\n",
      "   b \n",
      "     [-5.25956917]\n",
      "     [ 0.54027456]\n",
      "     [ 4.62615967]\n",
      "     [-5.25784683]\n",
      "   loss:  [[ 0.69573581]\n",
      " [ 0.31341541]\n",
      " [ 0.31587896]\n",
      " [ 0.69574028]]\n",
      "________________________________________________________________________________\n",
      "Epoch:  50000\n",
      "   y_estimated: \n",
      "     [ 0.00411439]\n",
      "     [ 0.99955994]\n",
      "     [ 0.99229115]\n",
      "     [ 0.00412001]\n",
      "   W: \n",
      "     [-0.41441455 -0.92060977]\n",
      "     [-1.52169275  1.73516226]\n",
      "   c: \n",
      "     [ 0.  0.]\n",
      "     [ 0.          1.08531272]\n",
      "     [ 0.  0.]\n",
      "     [ 0.         -0.81457621]\n",
      "   w: \n",
      "     [-1.03013289]\n",
      "     [ 2.54371715]\n",
      "   b \n",
      "     [-5.48914194]\n",
      "     [ 0.55372518]\n",
      "     [ 4.85763836]\n",
      "     [-5.48777103]\n",
      "   loss:  [[ 0.69520652]\n",
      " [ 0.31338006]\n",
      " [ 0.31534079]\n",
      " [ 0.69520926]]\n",
      "________________________________________________________________________________\n",
      "Epoch:  60000\n",
      "   y_estimated: \n",
      "     [ 0.00341566]\n",
      "     [ 0.99964428]\n",
      "     [ 0.99360472]\n",
      "     [ 0.00341957]\n",
      "   W: \n",
      "     [-0.41441455 -0.92970681]\n",
      "     [-1.52169275  1.75325859]\n",
      "   c: \n",
      "     [ 0.  0.]\n",
      "     [ 0.          1.11250615]\n",
      "     [ 0.  0.]\n",
      "     [ 0.         -0.82367325]\n",
      "   w: \n",
      "     [-1.03013289]\n",
      "     [ 2.57404518]\n",
      "   b \n",
      "     [-5.67596292]\n",
      "     [ 0.56434649]\n",
      "     [ 5.04578209]\n",
      "     [-5.67481565]\n",
      "   loss:  [[ 0.69485646]\n",
      " [ 0.31335738]\n",
      " [ 0.31498566]\n",
      " [ 0.69485843]]\n",
      "________________________________________________________________________________\n",
      "Epoch:  70000\n",
      "   y_estimated: \n",
      "     [ 0.00291927]\n",
      "     [ 0.99970251]\n",
      "     [ 0.99453729]\n",
      "     [ 0.00292215]\n",
      "   W: \n",
      "     [-0.41441455 -0.93721098]\n",
      "     [-1.52169275  1.76838124]\n",
      "   c: \n",
      "     [ 0.  0.]\n",
      "     [ 0.          1.13513315]\n",
      "     [ 0.  0.]\n",
      "     [ 0.         -0.83117741]\n",
      "   w: \n",
      "     [-1.03013289]\n",
      "     [ 2.59914541]\n",
      "   b \n",
      "     [-5.83349705]\n",
      "     [ 0.57310778]\n",
      "     [ 5.20433044]\n",
      "     [-5.83250856]\n",
      "   loss:  [[ 0.69460785]\n",
      " [ 0.31334171]\n",
      " [ 0.31473377]\n",
      " [ 0.69460934]]\n",
      "________________________________________________________________________________\n",
      "Epoch:  80000\n",
      "   y_estimated: \n",
      "     [ 0.0025489]\n",
      "     [ 0.99974507]\n",
      "     [ 0.99523288]\n",
      "     [ 0.00255109]\n",
      "   W: \n",
      "     [-0.41441455 -0.94364864]\n",
      "     [-1.52169275  1.78121114]\n",
      "   c: \n",
      "     [ 0.  0.]\n",
      "     [ 0.          1.15439999]\n",
      "     [ 0.  0.]\n",
      "     [ 0.         -0.83761507]\n",
      "   w: \n",
      "     [-1.03013289]\n",
      "     [ 2.62086535]\n",
      "   b \n",
      "     [-5.96954107]\n",
      "     [ 0.58052671]\n",
      "     [ 5.34123373]\n",
      "     [-5.96868038]\n",
      "   loss:  [[ 0.69442242]\n",
      " [ 0.31333026]\n",
      " [ 0.31454602]\n",
      " [ 0.6944235 ]]\n",
      "________________________________________________________________________________\n",
      "Epoch:  90000\n",
      "   y_estimated: \n",
      "     [ 0.00226171]\n",
      "     [ 0.9997775]\n",
      "     [ 0.99577183]\n",
      "     [ 0.00226344]\n",
      "   W: \n",
      "     [-0.41441455 -0.94926506]\n",
      "     [-1.52169275  1.79244184]\n",
      "   c: \n",
      "     [ 0.  0.]\n",
      "     [ 0.          1.17124724]\n",
      "     [ 0.  0.]\n",
      "     [ 0.        -0.8432315]\n",
      "   w: \n",
      "     [-1.03013289]\n",
      "     [ 2.6397686]\n",
      "   b \n",
      "     [-6.08936834]\n",
      "     [ 0.58688128]\n",
      "     [ 5.46173811]\n",
      "     [-6.08860254]\n",
      "   loss:  [[ 0.69427866]\n",
      " [ 0.31332153]\n",
      " [ 0.31440055]\n",
      " [ 0.69427955]]\n",
      "________________________________________________________________________________\n",
      "Epoch:  100000\n",
      "   y_estimated: \n",
      "     [ 0.00203266]\n",
      "     [ 0.99980289]\n",
      "     [ 0.99620193]\n",
      "     [ 0.00203402]\n",
      "   W: \n",
      "     [-0.41441455 -0.95422935]\n",
      "     [-1.52169275  1.8023752 ]\n",
      "   c: \n",
      "     [ 0.  0.]\n",
      "     [ 0.          1.18614507]\n",
      "     [ 0.  0.]\n",
      "     [ 0.         -0.84819579]\n",
      "   w: \n",
      "     [-1.03013289]\n",
      "     [ 2.6564579]\n",
      "   b \n",
      "     [-6.19637585]\n",
      "     [ 0.59251189]\n",
      "     [ 5.56945419]\n",
      "     [-6.19570589]\n",
      "   loss:  [[ 0.69416398]\n",
      " [ 0.31331471]\n",
      " [ 0.31428456]\n",
      " [ 0.69416469]]\n",
      "________________________________________________________________________________\n",
      "Elapsed time  99.42221600000002\n"
     ]
    }
   ],
   "source": [
    "# 6.1 Example: Learning XOR - GBC Book - Chapter 6 - pp. 166 to 171\n",
    "# Some parts where inspired by the blog post\n",
    "# Solving XOR with a Neural Network in TensorFlow\n",
    "# by Stephen OMAN\n",
    "# \n",
    "# https://github.com/StephenOman/TensorFlowExamples/blob/master/xor%20nn/xor_nn.py\n",
    "\n",
    "# Activation RELU + sigmoid for binary classification output + MSE loss function\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=[4,2], name = 'X')\n",
    "Y = tf.placeholder(tf.float32, shape=[4,1], name = 'Y')\n",
    "\n",
    "W = tf.Variable(tf.truncated_normal([2,2]), name = \"W\")\n",
    "w = tf.Variable(tf.truncated_normal([2,1]), name = \"w\")\n",
    "\n",
    "c = tf.Variable(tf.zeros([4,2]), name = \"c\")\n",
    "b = tf.Variable(tf.zeros([4,1]), name = \"b\")\n",
    "\n",
    "with tf.name_scope(\"hidden_layer\") as scope:\n",
    "    h = tf.nn.relu(tf.add(tf.matmul(X, W),c))\n",
    "\n",
    "with tf.name_scope(\"output\") as scope:\n",
    "    y_estimated = tf.sigmoid(tf.add(tf.matmul(h,w),b))\n",
    "\n",
    "with tf.name_scope(\"loss\") as scope:\n",
    "    loss = tf.reduce_mean(tf.squared_difference(y_estimated, Y)) \n",
    "\n",
    "# For better result with binary classifier, use cross entropy with a sigmoid\n",
    "#    loss = tf.nn.sigmoid_cross_entropy_with_logits(logits=y_estimated, labels=Y)\n",
    "\n",
    "# A naïve direct implementation of the loss function\n",
    "#     n_instances = X.get_shape().as_list()[0]\n",
    "#     loss = tf.reduce_sum(tf.pow(y_estimated - Y, 2))/ n_instances\n",
    "\n",
    "# In case of problem with gradient (exploding or vanishing gradient)perform gradient clipping\n",
    "#     loss = tf.reduce_sum(tf.pow(tf.clip_by_value(y_estimated,1e-10,1.0) - Y,2))/(n_instances)\n",
    "\n",
    "with tf.name_scope(\"train\") as scope:\n",
    "    train_step = tf.train.GradientDescentOptimizer(0.01).minimize(loss)\n",
    "\n",
    "INPUT_XOR = [[0,0],[0,1],[1,0],[1,1]]\n",
    "OUTPUT_XOR = [[0],[1],[1],[0]]\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "\n",
    "writer = tf.summary.FileWriter(\"./logs/xor_logs\", sess.graph)\n",
    "\n",
    "sess.run(init)\n",
    "\n",
    "t_start = time.clock()\n",
    "for epoch in range(100001):\n",
    "    sess.run(train_step, feed_dict={X: INPUT_XOR, Y: OUTPUT_XOR})\n",
    "    if epoch % 10000 == 0:\n",
    "        print(\"_\"*80)\n",
    "        print('Epoch: ', epoch)\n",
    "        print('   y_estimated: ')\n",
    "        for element in sess.run(y_estimated, feed_dict={X: INPUT_XOR, Y: OUTPUT_XOR}):\n",
    "            print('    ',element)\n",
    "        print('   W: ')\n",
    "        for element in sess.run(W):\n",
    "            print('    ',element)\n",
    "        print('   c: ')\n",
    "        for element in sess.run(c):\n",
    "            print('    ',element)\n",
    "        print('   w: ')\n",
    "        for element in sess.run(w):\n",
    "            print('    ',element)\n",
    "        print('   b ')\n",
    "        for element in sess.run(b):\n",
    "            print('    ',element)\n",
    "        print('   loss: ', sess.run(loss, feed_dict={X: INPUT_XOR, Y: OUTPUT_XOR}))\n",
    "t_end = time.clock()\n",
    "print(\"_\"*80)\n",
    "print('Elapsed time ', t_end - t_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0a2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
